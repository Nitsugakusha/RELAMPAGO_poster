{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y traducción de tweets de texto común a ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook generadora del dataset para utilizar en el análisis. Toma la base de datos, remueve las palabras y signos a ignorar, traduce los emojis a ASCII y guarda el dataset final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from TweetAnalysis import hermes\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargo la base de datos clasificada y genero las variables de importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW = pd.read_csv('base_corr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los tweets no clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('tweets.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSJ contiene al tweet, Tag si es meteorológico o no, Num el id del tweet, Who el nombre de quien los clasificó y User el id de quien twiteó (eso lo tuvimos que sacar de la base de datos original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSJ = TW['Twit']\n",
    "TAG = TW['Tag']\n",
    "NUM = TW['Num']\n",
    "WHO = TW['Nombre']\n",
    "USER = DATA['user_id'][np.asarray(NUM)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL = pd.DataFrame(list(zip(MSJ, TAG, NUM, USER, WHO)), columns = ['MSJ', 'TAG', 'NUN', 'USER', 'WHO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función Hermes: normaliza los tweets y borra el spam meteorológico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL = hermes(FINAL, 'MSJ', 'USER', 5)\n",
    "FINAL.to_csv(r'BaseOrt.csv', sep = ';', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrector ortográfico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mano :( Se puede hacer con Hunspell, si lo lográs instalar, suerte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORT = pd.read_csv('BaseOrt2.csv', sep = ';', header = None)\n",
    "ORT.columns = ['msj_ort']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "espaniol_stemmer = SnowballStemmer('spanish')\n",
    "ORT_stemm = []\n",
    "for i in ORT['msj_ort']:\n",
    "    msj = i.lower().split()\n",
    "    for j in range(len(msj)):\n",
    "        msj[j] = espaniol_stemmer.stem(msj[j])\n",
    "    i = ' '.join(msj)\n",
    "    ORT_stemm.append(i)\n",
    "ORT['msj_stemm'] = ORT_stemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción: le saco los plurales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORT_no_ese = []\n",
    "for i in ORT['msj_ort']:\n",
    "    msj = i.lower().split()\n",
    "    for j in range(len(msj)):\n",
    "        if (msj[j][-1] == 's') and (msj[j]!='mas'): msj[j]= msj[j][:-1]\n",
    "    i = ' '.join(msj)\n",
    "    ORT_no_ese.append(i)\n",
    "ORT['msj_noplural'] = ORT_no_ese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remuevo emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORT_no_emoji = []\n",
    "for i in ORT['msj_ort']:\n",
    "    msj = i.lower().split()\n",
    "    for j in range(len(msj)):\n",
    "        if '\\\\u' in msj[j]: msj[j] = ' '\n",
    "    i = ' '.join(msj)\n",
    "    ORT_no_emoji.append(i)\n",
    "ORT['msj_noemoji'] = ORT_no_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORT.to_csv(r'BaseRed.csv', sep = ';', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
