{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y traducción de tweets de texto común a ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook generadora del dataset para utilizar en el análisis. Toma la base de datos, remueve las palabras y signos a ignorar, traduce los emojis a ASCII y guarda el dataset final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargo la base de datos clasificada y genero las variables de importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW = pd.read_csv('base_corr.csv')\n",
    "TW = TW.sort_values(by = ['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('tweets.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSJ contiene al tweet, Tag si es meteorológico o no, Num el id del tweet, Who el nombre de quien los clasificó y User el id de quien twiteó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSJ = TW['Twit']\n",
    "TAG = TW['Tag']\n",
    "NUM = TW['Num']\n",
    "WHO = TW['Nombre']\n",
    "USER = DATA['user_id'][np.asarray(NUM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Borro los tweets de perfiles de spam meteorológico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_users = list(map(str,USER))\n",
    "\n",
    "A = list()\n",
    "for i in set(cant_users):\n",
    "    h = sum(np.char.count(cant_users, i))\n",
    "    if h >= 10: A.append(float(i))\n",
    "        \n",
    "for i in range(len(A)):\n",
    "    aux = np.where(USER == A[i])\n",
    "    MSJ = MSJ.drop(aux[0], axis = 0)\n",
    "    TAG = TAG.drop(aux[0], axis = 0)\n",
    "    NUM = NUM.drop(aux[0], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elimino las cosas innecesarias de msj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacion(mensaje):\n",
    "    \n",
    "    mensaje = list(mensaje)\n",
    "    lenght = len(mensaje)\n",
    "    \n",
    "    lista_ascii = [['\\\\xe1', '\\\\xe9', '\\\\xed', '\\\\xf3', '\\\\xfa', '\\\\xf1', '\\\\xd1', '\\\\xc1','\\\\xc9', '\\\\xcd', '\\\\xd3', \n",
    "                 '\\\\xda', '\\\\xa1', '\\\\xbf', '\\\\xdc', '\\\\xfc', '\\\\xb4', '\\\\xba', '\\\\xaa', '\\\\xb7','\\\\xc7','\\\\xe7',\n",
    "                 '\\\\xa8', '\\\\xb0C', '\\\\n', '\\\\xb0c', '\\\\xbb', 'xb0', '\\\\ufe0f'],['a', 'e', 'i', 'o', 'u', 'ñ', 'Ñ', 'A', 'E', 'I', 'O', 'U', '', '', 'Ü', 'ü', '', \n",
    "                           ' ', '', '','Ç','ç','', ' ', ' ', ' ', ' ', ' ', ' ']]\n",
    "    lista_simb = ['!','\\'', '\\\"', \"|\", '$', \"%\", \"&\", \"(\", \")\", \"=\", \"?\", \"+\",'/', \";\", \"_\", \"-\", \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\", '*',',', '.', ':', '#']\n",
    "    \n",
    "    ign_palabras = ['mbar','cdu','s','la','lo','st','que','este', 'me','t','p','el','weathercloud','en','h','temp','hpa','km','mm',\"su\",\"vos\",'que',\"re\",\"xq\",\"le\",\"te\",\"tu\",\"soy\",\"sos\",\"mi\",\"da\",\"o\",\"x\",\"les\",\"me\",\"d\",\"q\", \"como\", \"lo\", \"los\", \"mi\", \"son\", \"a\", \"el\", \n",
    "                    \"un\",\"la\", \"una\",\"en\",\"por\",\"para\", 'las',\"ante\", \"al\", 'me',\"rt\", \"del\", \"y\", \"se\", \"de\", \"que\", \"sus\", \"ha\", \"es\", \"con\", \"esta\"]\n",
    "    \n",
    "    for i in range(lenght):\n",
    "        \n",
    "        ## Convierto mayúsculas en minúsculas\n",
    "        mensaje[i] = mensaje[i].lower()\n",
    "        \n",
    "        ## Saco las menciones y otras cosas\n",
    "        txt = mensaje[i].split()\n",
    "        \n",
    "        for j in range(len(txt)):\n",
    "            if ('@' in txt[j]) and ('RELAMPAGO2018' not in txt[j]) and ('RELAMPAGO_edu' not in txt[j]) or ('jaj' in txt[j]) or ('https' in txt[j]): txt[j]=''\n",
    "        \n",
    "        \n",
    "        mensaje[i] = \" \".join(txt)\n",
    "        \n",
    "        ## Reemplazo símbolos\n",
    "        for h in range(len(lista_simb)):\n",
    "            mensaje[i] = mensaje[i].replace(lista_simb[h], ' ')\n",
    "            \n",
    "        ## Convierto el msj a ASCII, reemplazo los errores de decodificación y agrego un espacio antes de cada decodificación\n",
    "        mensaje[i] = mensaje[i].encode('unicode-escape').decode('ASCII')+\" \"     \n",
    "        \n",
    "        for j in range(len(lista_ascii[0])):\n",
    "            mensaje[i] = mensaje[i].replace(lista_ascii[0][j], lista_ascii[1][j])\n",
    "        \n",
    "        mensaje[i] = mensaje[i].replace('\\\\', ' \\\\')\n",
    "        \n",
    "        for j in range(len(ign_palabras)):\n",
    "            mensaje[i] = mensaje[i].replace(\" \"+ign_palabras[j]+\" \", ' ')\n",
    "        \n",
    "    return(mensaje)\n",
    "msj = normalizacion(MSJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### División en datasets de  _train_ y _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0.8\n",
    "\n",
    "## Toma los tweets, los asocia con su tag y luego los divide en cada una de las palabras que los componen\n",
    "## zip() itera tuplas\n",
    "\n",
    "train = [(i.split(),j) for i,j in zip(list(msj)[1:int(len(msj)*0.8)],list(TAG)[1:int(len(msj)*0.8)])]\n",
    "\n",
    "test = [(i.split(),j) for i,j in zip(list(msj)[int(len(msj)*0.8)+1:len(msj)],list(TAG)[int(len(msj)*0.8)+1:len(msj)])]\n",
    "\n",
    "## Guardo los labels, (en este caso 1 y 0) en un diccionario\n",
    "label_to_ix = { 1: 0, 0: 1 } \n",
    "\n",
    "# 1 = Meteorológico\n",
    "# 0 = No meteorológico\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generación del diccionario con el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "\n",
    "# El diccionario tiene que tener todas las palabras posibles\n",
    "datos = train + test\n",
    "\n",
    "## Cada palabra que encuentra la guarda en un diccionario\n",
    "for sent, _ in datos:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
